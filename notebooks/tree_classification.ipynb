{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/iu5git/Deep-learning/blob/main/notebooks/tree_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Лабораторная работа\n",
    "\n"
   ],
   "metadata": {
    "id": "ZpsOBMUGgYjD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Теория\n",
    "\n",
    "Облако точек - это самый простой способ представления различных объектов в виде неупорядоченного набора точек в трехмерной плоскости. Такие данные можно получить с помощью сканирования предметов или их структуры с помощью 3D-датчиков, например LiDAR. Качественные облака точек с высокой точностью измерения позволяют представить цифровую версию реального мира.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://www.codeproject.com/KB/openGL/839389/bunny_points.PNG' />\n",
    "<figcaption>Стэндфордский кролик</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Как правило, при глубоком обучении трехмерного облака точек необходимо решить две задачи: классификацию и сегментацию.\n",
    "Основная проблема работы с облаком точек заключается в том, что типичная сверточная архитектура требует упорядоченный формат входных данных (например, изображение). Поскольку облако точек не является таким, общепринятые подходы заключаются в преобразовании данных в обычную 3D-воксельную сетку или проекцию.\n",
    "\n",
    "В данной лабораторной работе используется архитектура PointNet. Эта модель использует неупорядоченные облака точек и может выполнять классификацию и сегментацию объектов, а также семантический анализ сцены.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='http://stanford.edu/~rqi/pointnet/images/teaser.jpg' />\n",
    "<figcaption>Применение PointNet</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "В сети есть 3 ключевых модуля: слой Max Pooling, принимающий n векторов входных данных и выводящий новый вектор, две сети трансформации с многослойным персептроном (MLP) с размерами (64,64) и (64,128,1024) и две сети для предсказания с обученной матрицей преобразования T-Net.\n",
    "\n",
    "PointNet изучает характеристики индивидуальной точки с помощью MLP и объединяет все их характеристики с помощью симметричной функции для выполнения классификации объектов и их сегментации на части. \n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='http://stanford.edu/~rqi/pointnet/images/pointnet.jpg' />\n",
    "<figcaption>Архитектура PointNet</figcaption></center>\n",
    "</figure>\n"
   ],
   "metadata": {
    "id": "NSmZDQ1xiLtD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Импорт библиотек"
   ],
   "metadata": {
    "id": "EaFo0Hz9hGG2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!apt install subversion"
   ],
   "metadata": {
    "id": "iRZsWtLs9Rv4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import regularizers\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "tf.random.set_seed(42)"
   ],
   "metadata": {
    "id": "MzXctZ3SFxMo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка набора данных\n",
    "\n",
    "Для выполнения лабораторной работы необходимо скачать файл формата h5. Поскольку каждая точка одного облака точек содержит информацию о 3 различных координатах, данный формат удобен для хранения и работы с таким большим объемом данных.\n",
    "\n",
    "Файл выбирается следующим образом:\n",
    "\n",
    "v1 - нечетный вариант, v2 - четный вариант"
   ],
   "metadata": {
    "id": "Yy3i39KThRrf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!svn checkout https://github.com/iu5git/Deep-learning/trunk/datasets/lidar"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_5-ribOjjVM",
    "outputId": "40a30fdf-a977-44af-ba07-b17c061995f0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A    lidar/v1.h5\n",
      "A    lidar/v2.h5\n",
      "Checked out revision 119.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "DATA_DIR = \"/content/lidar\""
   ],
   "metadata": {
    "id": "Mnu1zLxOF56T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "h5f = h5py.File(os.path.join(DATA_DIR, \"v1.h5\"),'r') # файл по своему варианту\n",
    "X = h5f.get('dataset_X')[:]\n",
    "Y = h5f.get('dataset_Y').asstr()[:]\n",
    "h5f.close()"
   ],
   "metadata": {
    "id": "y2v-oxqBF8T0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "NUM_POINTS = 4096\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "CLASSES = {\n",
    "    0:'Рябина', \n",
    "    1:'Ель', \n",
    "    2:'Сосна', \n",
    "    3:'Дуб',\n",
    "    4:'Береза'\n",
    "}"
   ],
   "metadata": {
    "id": "F9kBFALXGM5f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Y = np.array([list(CLASSES.values()).index(y) for y in Y])\n",
    "indexes = []\n",
    "[indexes.append(y) for y in list(Y) if y not in indexes]\n",
    "indexes.sort()\n",
    "CLASS_MAP = {i: CLASSES[k] for (k, i) in (zip(indexes, range(len(indexes))))}\n",
    "\n",
    "#кол-во классов по своему варианту\n",
    "NUM_CLASSES = len(CLASS_MAP)\n",
    "\n",
    "for (k, i) in (zip(indexes, range(len(indexes)))):\n",
    "  Y[Y == k] = i"
   ],
   "metadata": {
    "id": "6du8biS_oSXH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "points = X[50]\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "myo5gyyzjGf_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = {'Кол-во деревьев': list(pd.value_counts(Y).sort_index()),\n",
    "        'Деревья': list(CLASS_MAP.values())}\n",
    "df = pd.DataFrame(data).set_index('Деревья')\n",
    "ax = df.plot.bar()"
   ],
   "metadata": {
    "id": "1_q-IBner6O_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Разбиение выборки на тренировочную и тестовую"
   ],
   "metadata": {
    "id": "fWor6G15h-eb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "skf = StratifiedKFold(n_splits=5).split(X, Y)\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "X_augment = []\n",
    "y_augment = []\n",
    "\n",
    "for i in range(4):\n",
    "    point_select = []\n",
    "    for x in X_train:\n",
    "        idx = np.random.choice(X_train.shape[0], size=NUM_POINTS, replace=True)\n",
    "        point_select.append(x[idx])\n",
    "    point_select = np.array(point_select)        \n",
    "    point_select = point_select + np.random.normal(0, 0.005, point_select.shape)\n",
    "    X_augment.append(point_select)\n",
    "    y_augment.append(y_train)\n",
    "\n",
    "X_augment = np.array(X_augment)\n",
    "y_augment = np.array(y_augment)\n",
    "X_augment = np.reshape(X_augment,(X_augment.shape[0] * X_augment.shape[1], NUM_POINTS, 3))\n",
    "y_augment = np.reshape(y_augment,(-1))"
   ],
   "metadata": {
    "id": "gLkdiCvMHy7m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_augment, y_augment))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(X_augment)).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "id": "DwVpeG2DlBsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Построение модели\n",
    "\n",
    "Каждый сверточный и полносвязный слой (не включая конечных слоев) состоит из Convolution / Dense -> Batch Normalization -> ReLU Activation."
   ],
   "metadata": {
    "id": "Zf9mbCHkhl-S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)"
   ],
   "metadata": {
    "id": "pkfkKcYMIVKG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "PointNet состоит из двух основных компонентов: основная сеть MLP (многослойный перцептрон) и трансформаторная сеть T-net."
   ],
   "metadata": {
    "id": "gcVm4reEW7K2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))"
   ],
   "metadata": {
    "id": "4mPCuwVhlJhX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Определим общую функцию для построения слоев T-net."
   ],
   "metadata": {
    "id": "ALXzk7HgXPJ-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def tnet(inputs, num_features):\n",
    "\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    \n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
   ],
   "metadata": {
    "id": "kfwJxKszlN9x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
    "\n",
    "x = tnet(inputs, 3)\n",
    "x = conv_bn(x, 32)\n",
    "x = conv_bn(x, 32)\n",
    "x = tnet(x, 32)\n",
    "x = conv_bn(x, 32)\n",
    "x = conv_bn(x, 64)\n",
    "x = conv_bn(x, 512)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = dense_bn(x, 256)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = dense_bn(x, 128)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "QBHNLL5OlQOz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение модели"
   ],
   "metadata": {
    "id": "obG5XF57igJY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
   ],
   "metadata": {
    "id": "z84IF1Z2ilf1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Визуализация результатов"
   ],
   "metadata": {
    "id": "LDOuMD_Min_2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "QnsXT6ryldeR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Cc1VzgsFl2u2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = test_dataset.take(1)\n",
    "\n",
    "points, labels = list(data)[0]\n",
    "points = points[:8, ...]\n",
    "labels = labels[:8, ...]\n",
    "\n",
    "preds = model.predict(points)\n",
    "preds = tf.math.argmax(preds, -1)\n",
    "\n",
    "points = points.numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(4, 2, i + 1, projection=\"3d\")\n",
    "    ax.scatter(points[i, :, 0], points[i, :, 1], points[i, :, 2])\n",
    "    ax.set_title(\n",
    "        \"pred: {:}, label: {:}\".format(\n",
    "            CLASS_MAP[preds[i].numpy()], CLASS_MAP[labels.numpy()[i]]\n",
    "        )\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "LRijfqB-l5iw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ],
   "metadata": {
    "id": "Db4c9RFrmHMV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = test_dataset.take(1)\n",
    "points, labels = list(data)[0]\n",
    "points = points[:, ...]\n",
    "labels = labels[:, ...]\n",
    "\n",
    "preds = model.predict(points)\n",
    "preds = tf.math.argmax(preds, -1)\n",
    "\n",
    "cm = confusion_matrix(y_true=labels, y_pred=preds)"
   ],
   "metadata": {
    "id": "gfP5nerImBc3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_confusion_matrix(cm=cm, classes=CLASS_MAP.values(), title='Confusion Matrix')"
   ],
   "metadata": {
    "id": "fWuxFwhhmJTE"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
