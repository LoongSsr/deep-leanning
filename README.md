# Разработка Нейросетевых Систем
Репозиторий курса Разработка Нейросетевых Систем

### Отчеты
Для **допуска** к экзамену необходимо подготовить отчет по каждому модулю. Отчеты по разделам содержат ваши результаты лабораторных работ и ДЗ. Фиксируйте этапы вашей работы, ваши вариации гиперпараметров и моделей, выводы к чему это приводит. На защиту приносите сразу текущую версию отчета.

Отчеты отправлять на почту ``aikanev@bmstu.ru``

## Лекции
[Записи лекций](https://www.youtube.com/watch?v=07iFp7YcmOg&list=PLLELLTvDgUQ_d9eUj_3XVpAdGByuU37kT) Youtube

1. [Лекция №1](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_1_DL.pdf)
2. [Лекция №2](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_2_CNN.pdf)
3. [Лекция №3](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_3_Data_Augmentation.pdf)
4. [Лекция №4](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_4_Transfer_Learning.pdf)
5. [Лекция №5](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_5_Autoencoders.pdf)
6. [Лекция №6](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_6_RNN.pdf)
7. Лекция №7
8. Лекция №8

## Дополнительные лекции
- [Обработка естественного языка](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_7_NLP.pdf)
- [Трансформеры](https://github.com/iu5git/Deep-learning/blob/main/lectures/Lection_8_Transformer.pdf)

## Рубежный контроль
1. Рубежный контроль №1
2. Рубежный контроль №2

## Лабораторные работы

Выражаем благодарность Ишкову Денису за подготовку лабораторных работ

1. [Лабораторная работа №1](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab1.ipynb)
2. [Лабораторная работа №2](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab2.ipynb)
3. [Лабораторная работа №3](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab3.ipynb)
4. [Лабораторная работа №4](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab4.ipynb)
5. [Лабораторная работа №5](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab5.ipynb)
6. [Лабораторная работа №6](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab6.ipynb)
7. Лабораторная работа №7
8. Лабораторная работа №8

## Домашнее задание

- [Домашнее задание №1](https://github.com/iu5git/Deep-learning/blob/main/homework/homework1.md)
- Домашнее задание №2

## Дополнительные материалы
- [Генерация текста](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab7.ipynb)
- [Машинный перевод](https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab8.ipynb)
- [Телеграм бот](https://github.com/iu5git/Deep-learning/blob/main/homework2/task.md)

## Вопросы к экзамену
1.	Опишите алгоритм обучения с учителем.
2.	Устройство нейрона, формула вычисления значения. Объясните принцип его работы. 
3.	Многослойный персептрон, архитектура, достоинства и недостатки.
4.	Виды активационных функций, назначение.
5.	Количество нейронов, связей, параметров в полносвязной нейронной сети.
6.	Эпоха, батч, итерация обучения.
7.	Алгоритм обратного распространения ошибки.
8.	Алгоритм оптимизации AdaGrad.
9.	Алгоритм оптимизации RMSProp.
10.	Алгоритм оптимизации с моментом.
11.	Алгоритм оптимизации Adam.
12.	Алгоритм оптимизации стохастического градиентного спуска.
13.	Что такое гиперпараметры? Приведите примеры, оптимальные значения гиперпараметров.
14.	Что такое свертка, как она применяется в нейронных сетях?
15.	Свойства свертки.
16.	Количество нейронов, связей и параметров в сверточном слое.
17.	Опишите структуру набора данных.
18.	Аугментация данных
19.	Что такое stride, padding? Варианты.
20.	Дайте определение пулинга. Примеры
21.	Дайте определение регуляризации, dropout.
22.	Переобучение и недообучение нейронной сети.
23.	Дайте определение функции потери.
24.	Кросс-энтропия, как функция потери.
25.	Метод наименьших квадратов, как функция потери.
26.	Архитектура трансформер.
27.	Механизм внимания.
28.	Архитектура seq2seq.
29.	Понятие временного ряда (ВР). Примеры ВР. Цель анализа ВР. 
30.	Опишите задачи регрессии и классификации.
31.	Авторегрессионная модель. Преимущества и недостатки.
32.	Перенос обучения, дообучение. Принцип, преимущества.
33.	Способы сокращения размерности карты признаков.
34.	Архитектура автоэнкодера.
35.	Понятие рекуррентных нейронных сетей. Структурная схема RNN.
36.	Особенности обучения рекуррентных нейросетей. Проблема затухающих и взрывных градиентов.
37.	LSTM сети. Преимущества LSTM по сравнению с RNN. 
38.	Решение проблемы исчезающих градиентов в LSTM. Начальная инициализация параметров.
39.	Возможные модификации LSTM. Их преимущества и недостатки.
40.	GRU (Gated recurrent unit) сети.
41.	Для чего необходима предобработка текста? Перечислите ее виды. 
42.	Что такое N-граммы?
43.	Что такое стемминг, лемматизация? Примеры и применение. 
44.	Представление текста в виде векторной модели (vector space model)? Как вычисляются коэффициенты для bag-of-words и TF-IDF? 
45.	В чем заключается векторное представление (embedding)? Примеры применения.
46.	Преимущества и недостатки стохастического и пакетного градиентного спуска.
47.	Что такое ONNX, Pytorch?
48.	Оценка точности классификации F1-score, формула расчета и составляющие. 
49.	Для чего используются сверточные слои нейронных сетей при анализе текста? 
50.	Какие свойства рекуррентных и LSTM слоев обуславливают их широкое применение для анализа текста?
